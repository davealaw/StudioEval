# StudioEval

**StudioEval** is a lightweight evaluation framework for benchmarking LLMs using [LM Studio](https://lmstudio.ai) as the model backend. It supports custom GGUF models and works seamlessly with MLX and llama.cpp.

## ğŸš€ Features

- Modular dataset loaders (custom and Hugging Face)
- Dynamic dataset discovery and invocation
- Supports GGUF, MLX, and other local models via LM Studio API
- Clear scoring and prompt templating utilities
- Easily extensible for new datasets or evaluation strategies

## ğŸ“ Project Structure

```
studioeval/
â”œâ”€â”€ studioeval.py              # Main entry point (formerly llm_lm_eval_lmstudio.py)
â”œâ”€â”€ eval_datasets/             # All dataset loaders
â”‚   â”œâ”€â”€ loader.py              # Dynamic loading logic
â”‚   â”œâ”€â”€ custom/
â”‚   â””â”€â”€ huggingface/
â”œâ”€â”€ models/                    # Model metadata, endpoint configs, etc.
â”œâ”€â”€ utils/                     # Prompt formatting, scoring, helpers
â”œâ”€â”€ config/                    # Shared constants
â”œâ”€â”€ README.md
â””â”€â”€ pyproject.toml
```

## âœ… Getting Started

1. Clone the repository
2. Install dependencies:

   ```bash
   pip install -e .
   ```

3. Run an evaluation:

   ```bash
   python studioeval.py --dataset truthfulqa --model your-model-name
   ```

## ğŸ› ï¸ License

MIT License
