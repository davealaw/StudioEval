
default_datasets_to_run = [
  {
    "eval_type": "grammar",
    "dataset_path": "eval_datasets/custom/data/grammar_dataset.jsonl",
    "dataset_name": "grammar",
    "split": None,
    "seed": None,
    "sample_size": None
  },
  {
    "eval_type": "custom_mcq",
    "dataset_path": "eval_datasets/custom/data/creative_writing_mcq_dataset.jsonl",
    "dataset_name": "creative_writing",
    "split": None,
    "seed": None,
    "sample_size": None
  },
  {
    "eval_type": "custom_mcq",
    "dataset_path": "eval_datasets/custom/data/data_analysis_mcq_dataset.jsonl",
    "dataset_name": "data_analysis_mcq",
    "subset": None,
    "split": None,
    "seed": None,
    "sample_size": None
  },
  {
    "eval_type": "custom_mcq",
    "dataset_path": "eval_datasets/custom/data/coding_mcq_dataset.jsonl",
    "dataset_name": "coding_mcq",
    "subset": None,
    "split": None,
    "seed": None,
    "sample_size": None
  },
  {
    "eval_type": "math",
    "dataset_path": "eval_datasets/custom/data/elementary_math_dataset.jsonl",
    "dataset_name": "elementary_math",
    "subset": None,
    "split": None,
    "seed": None,
    "sample_size": None
  },
  {
    "eval_type": "math",
    "dataset_path": "eval_datasets/custom/data/hs_lower_university_math_dataset.jsonl",
    "dataset_name": "hs_math",
    "subset": None,
    "split": None,
    "seed": None,
    "sample_size": None
  },
  {
    "eval_type": "gsm8k",
    "dataset_path": "tinyBenchmarks/tinyGSM8k",
    "dataset_name": "tinyGSM8k",
    "subset": None,
    "split": None,
    "seed": None,
    "sample_size": None
  },
  {
    "eval_type": "arc",
    "dataset_path": "tinyBenchmarks/tinyAI2_arc",
    "dataset_name": "tinyARC",
    "subset": None,
    "split": None,
    "seed": None,
    "sample_size": None
  },
  {
    "eval_type": "mmlu",
    "dataset_path": "tinyBenchmarks/tinyMMLU",
    "dataset_name": "tinyMMLU",
    "subset": None,
    "split": None,
    "seed": None,
    "sample_size": None
  },
  {
    "eval_type": "commonsenseqa",
    "dataset_path": "tau/commonsense_qa",
    "dataset_name": "commonsenseqa",
    "subset": None,
    "split": None,
    "seed": None,
    "sample_size": None
  },
  {
    "eval_type": "logiqa",
    "dataset_path": "lucasmccabe/logiqa",
    "dataset_name": "logiqa",
    "subset": None,
    "split": None,
    "seed": None,
    "sample_size": None
  },
  {
    "eval_type": "truthfulqa",
    "dataset_path": "tinyBenchmarks/tinyTruthfulQA",
    "dataset_name": "tinyTruthfulQA",
    "subset": None,
    "split": None,
    "seed": None,
    "sample_size": None
  },
  {
    "eval_type": "winogrande",
    "dataset_path": "tinyBenchmarks/tinyWinogrande",
    "dataset_name": "tinyWinogrande",
    "split": None,
    "seed": None,
    "sample_size": None
  },
  {
    "eval_type": "hellaswag",
    "dataset_path": "tinyBenchmarks/tinyHellaswag",
    "dataset_name": "tinyHellaswag",
    "split": None,
    "seed": None,
    "sample_size": None
  }    
]