[
  {
    "eval_type": "custom_mcq",
    "dataset_path": "eval_datasets/custom/data/research_mcq_dataset.jsonl",
    "dataset_name": "research_mcq",
    "subset": null,
    "split": null,
    "seed": null,
    "sample_size": null
  },
  {
    "eval_type": "mmlu",
    "dataset_path": "tinyBenchmarks/tinyMMLU",
    "dataset_name": "tinyMMLU",
    "split": null,
    "seed": null,
    "sample_size": null
  },
  {
    "eval_type": "commonsenseqa",
    "dataset_path": "tau/commonsense_qa",
    "dataset_name": "commonsenseqa",
    "split": null,
    "seed": null,
    "sample_size": null
  },
  {
    "eval_type": "logiqa",
    "dataset_path": "lucasmccabe/logiqa",
    "dataset_name": "logiqa",
    "split": null,
    "seed": null,
    "sample_size": null
  },
  {
    "eval_type": "truthfulqa",
    "dataset_path": "tinyBenchmarks/tinyTruthfulQA",
    "dataset_name": "tinyTruthfulQA",
    "split": null,
    "seed": null,
    "sample_size": null
  }
]