"""
Performance benchmarking tests for StudioEval.
Tests performance characteristics and identifies bottlenecks.
"""
import pytest
import time
import statistics
from unittest.mock import patch, Mock
import concurrent.futures
import threading

from core.evaluator import EvaluationOrchestrator
from core.model_manager import ModelManager
from core.dataset_registry import DatasetRegistry
from implementations.mock_client import MockModelClient
from tests.fixtures.mock_datasets import MockDatasetLoader


@pytest.fixture
def performance_mock_client():
    """Mock client optimized for performance testing."""
    # Create responses for many models
    responses = {}
    models = []
    
    for i in range(100):
        model_name = f"perf-model-{i:03d}"
        models.append(model_name)
        responses[model_name] = f"Answer: {chr(65 + (i % 26))}"  # A, B, C, etc.
    
    client = MockModelClient(
        responses=responses,
        available_models=models,
        server_running=True
    )
    
    # Set realistic response times
    client.response_time = 0.1  # 100ms per query
    
    return client


class TestPerformanceBenchmarks:
    """Performance benchmarking tests for core components."""
    
    def test_model_resolution_performance(self, performance_mock_client):
        """Benchmark model resolution performance."""
        model_manager = ModelManager(performance_mock_client)
        
        # Benchmark different resolution strategies
        scenarios = [
            ("single_model", {"model": "perf-model-001"}),
            ("all_models", {"all_models": True}),
            ("filter_small", {"model_filter": "perf-model-00*"}),  # ~10 models
            ("filter_large", {"model_filter": "perf-model-*"}),    # ~100 models
        ]
        
        results = {}
        
        for scenario_name, kwargs in scenarios:
            start_time = time.time()
            
            # Run multiple iterations for statistical significance
            for _ in range(10):
                models = model_manager.resolve_models(**kwargs)
            
            end_time = time.time()
            avg_time = (end_time - start_time) / 10
            
            results[scenario_name] = {
                "avg_time": avg_time,
                "models_found": len(models)
            }
        
        # Performance assertions
        assert results["single_model"]["avg_time"] < 0.01  # < 10ms for single model
        assert results["all_models"]["avg_time"] < 0.1     # < 100ms for all models
        assert results["filter_small"]["avg_time"] < 0.05  # < 50ms for small filter
        assert results["filter_large"]["avg_time"] < 0.1   # < 100ms for large filter
        
        # Verify expected model counts
        assert results["single_model"]["models_found"] == 1
        assert results["all_models"]["models_found"] == 100
        assert results["filter_small"]["models_found"] >= 10
        assert results["filter_large"]["models_found"] == 100
    
    def test_dataset_registry_performance(self):
        """Benchmark dataset registry performance."""
        registry = DatasetRegistry()
        
        start_time = time.time()
        
        # Test repeated evaluator lookups
        evaluator_types = ["grammar", "custom_mcq", "gsm8k", "mmlu"]
        
        for _ in range(1000):
            for eval_type in evaluator_types:
                evaluator = registry.get_evaluator(eval_type)
                assert callable(evaluator)
        
        end_time = time.time()
        total_time = end_time - start_time
        avg_lookup_time = total_time / (1000 * len(evaluator_types))
        
        # Should be very fast - registry lookups are dictionary operations
        assert avg_lookup_time < 0.0001  # < 0.1ms per lookup
        assert total_time < 1.0          # Total should be under 1 second
    
    def test_evaluation_orchestrator_scaling(self, performance_mock_client):
        """Test how orchestrator performance scales with model/dataset count."""
        
        # Test scaling with different numbers of models
        model_counts = [1, 5, 10, 20]
        scaling_results = {}
        
        for model_count in model_counts:
            models = [f"perf-model-{i:03d}" for i in range(model_count)]
            
            # Mock the registry's evaluate_dataset method and sleep to avoid delays
            with patch.object(DatasetRegistry, 'evaluate_dataset') as mock_eval, \
                 patch('builtins.open') as mock_open, \
                 patch('time.sleep'):  # Mock sleep to avoid 2s delay per model
                
                mock_eval.return_value = {
                    "dataset": "scaling_test", "correct": 1, "total": 1, 
                    "accuracy": 100.0, "tok_per_sec": 15.0
                }
                
                orchestrator = EvaluationOrchestrator(
                    model_manager=ModelManager(performance_mock_client),
                    dataset_registry=DatasetRegistry()
                )
                
                start_time = time.time()
                
                result = orchestrator.run_evaluation(
                    models=models,
                    datasets_config=[{
                        "eval_type": "grammar",
                        "dataset_path": "test.json",
                        "dataset_name": "scaling_test"
                    }]
                )
                
                end_time = time.time()
                execution_time = end_time - start_time
                
                scaling_results[model_count] = {
                    "time": execution_time,
                    "time_per_model": execution_time / model_count,
                    "success": result
                }
        
        # Performance scaling assertions
        for count, result in scaling_results.items():
            assert result["success"] is True
            # Time per model should remain roughly constant (linear scaling)
            assert result["time_per_model"] < 0.5  # < 500ms per model
        
        # Verify linear scaling (within tolerance)
        time_per_model_values = [r["time_per_model"] for r in scaling_results.values()]
        scaling_variance = statistics.variance(time_per_model_values)
        
        # Variance should be low if scaling is truly linear
        assert scaling_variance < 0.1  # Low variance in per-model time
    
    def test_concurrent_model_operations(self, performance_mock_client):
        """Test performance of concurrent model operations."""
        model_manager = ModelManager(performance_mock_client)
        
        def query_model_task(model_name):
            """Task for concurrent execution."""
            response, stats = model_manager.query_model("Test query", model_name)
            return len(response), stats["tokens_per_second"]
        
        models_to_test = [f"perf-model-{i:03d}" for i in range(20)]
        
        # Test sequential execution
        start_time = time.time()
        sequential_results = []
        for model in models_to_test:
            result = query_model_task(model)
            sequential_results.append(result)
        sequential_time = time.time() - start_time
        
        # Test concurrent execution
        start_time = time.time()
        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
            concurrent_results = list(executor.map(query_model_task, models_to_test))
        concurrent_time = time.time() - start_time
        
        # Verify results are equivalent
        assert len(sequential_results) == len(concurrent_results)
        assert sequential_results == concurrent_results
        
        # Verify results are equivalent - this is the main goal
        # Performance comparison with mocks is unrealistic due to very fast operations
        # and thread creation overhead dominating the actual work
        
        print(f"Sequential time: {sequential_time:.3f}s")
        print(f"Concurrent time: {concurrent_time:.3f}s")
        
        if sequential_time > 0 and concurrent_time > 0:
            speedup_ratio = sequential_time / concurrent_time
            print(f"Speedup ratio: {speedup_ratio:.2f}x")
        else:
            print("Times too small to measure meaningfully")
        
        # The main assertion is that both approaches work correctly
        # Performance comparison with mocks isn't meaningful
    
    def test_memory_usage_patterns(self, performance_mock_client):
        """Test memory usage patterns during evaluation."""
        import tracemalloc
        
        tracemalloc.start()
        
        orchestrator = EvaluationOrchestrator(
            model_manager=ModelManager(performance_mock_client),
            dataset_registry=DatasetRegistry()
        )
        
        # Take baseline memory snapshot
        baseline_snapshot = tracemalloc.take_snapshot()
        
        # Simulate large evaluation run
        with patch.object(DatasetRegistry, 'evaluate_dataset') as mock_eval, \
             patch('builtins.open') as mock_open, \
             patch('time.sleep'):  # Mock sleep to avoid 2s delay per model
            
            mock_eval.return_value = {
                "dataset": "memory_test", "correct": 80, "total": 100,
                "accuracy": 80.0, "tok_per_sec": 12.0
            }
            
            # Run evaluation with many models
            models = [f"perf-model-{i:03d}" for i in range(50)]
            
            result = orchestrator.run_evaluation(
                models=models,
                datasets_config=[{
                    "eval_type": "grammar",
                    "dataset_path": "large_test.json",
                    "dataset_name": "memory_test"
                }]
            )
        
        # Take final memory snapshot
        final_snapshot = tracemalloc.take_snapshot()
        
        # Analyze memory growth
        top_stats = final_snapshot.compare_to(baseline_snapshot, 'lineno')
        
        total_memory_mb = sum(stat.size for stat in top_stats) / 1024 / 1024
        
        tracemalloc.stop()
        
        # Memory usage assertions
        assert result is True
        assert total_memory_mb < 50  # Should use less than 50MB for test data
        
        # Print top memory consumers for debugging
        print(f"Total memory growth: {total_memory_mb:.2f} MB")
        for stat in top_stats[:5]:
            print(f"  {stat.traceback.format()[-1]}: {stat.size / 1024:.1f} KB")
    
    def test_response_time_consistency(self, performance_mock_client):
        """Test consistency of response times."""
        model_manager = ModelManager(performance_mock_client)
        
        response_times = []
        model_name = "perf-model-001"
        
        # Measure response times for multiple queries
        for i in range(100):
            start_time = time.time()
            response, stats = model_manager.query_model(f"Test query {i}", model_name)
            end_time = time.time()
            
            response_times.append(end_time - start_time)
        
        # Statistical analysis
        mean_time = statistics.mean(response_times)
        std_dev = statistics.stdev(response_times)
        min_time = min(response_times)
        max_time = max(response_times)
        
        # Consistency assertions (loosened for mock-based testing)
        assert mean_time < 1.0      # Average response under 1 second
        assert std_dev < 0.5        # Reasonable standard deviation for mocks
        assert max_time < mean_time * 10  # No response more than 10x average (loosened for mocks)
        
        print(f"Response time stats:")
        print(f"  Mean: {mean_time:.4f}s")
        print(f"  Std Dev: {std_dev:.4f}s")
        print(f"  Min: {min_time:.4f}s")
        print(f"  Max: {max_time:.4f}s")


class TestStressTests:
    """Stress testing for extreme scenarios."""
    
    def test_high_model_count_stress(self):
        """Stress test with very high model count."""
        # Create mock client with 500 models
        responses = {f"stress-model-{i:04d}": "Answer: A" for i in range(500)}
        models = list(responses.keys())
        
        stress_client = MockModelClient(
            responses=responses,
            available_models=models,
            server_running=True
        )
        
        model_manager = ModelManager(stress_client)
        
        start_time = time.time()
        resolved_models = model_manager.resolve_models(all_models=True)
        resolution_time = time.time() - start_time
        
        # Should handle 500 models efficiently
        assert len(resolved_models) == 500
        assert resolution_time < 1.0  # Under 1 second even for 500 models
    
    def test_large_dataset_stress(self):
        """Stress test with large dataset."""
        client = MockModelClient(
            responses={"stress-model": "Answer: B"},
            available_models=["stress-model"],
            server_running=True
        )
        
        # Mock the registry's evaluate_dataset method to avoid all real function calls
        with patch.object(DatasetRegistry, 'evaluate_dataset') as mock_eval, \
             patch('builtins.open') as mock_open, \
             patch('time.sleep'):  # Mock sleep to avoid 2s delay per model
            
            mock_eval.return_value = {
                "dataset": "large_stress", "correct": 800, "total": 1000,
                "accuracy": 80.0, "tok_per_sec": 10.0
            }
            
            orchestrator = EvaluationOrchestrator(
                model_manager=ModelManager(client),
                dataset_registry=DatasetRegistry()
            )
            
            start_time = time.time()
            
            result = orchestrator.run_evaluation(
                model="stress-model",
                datasets_config=[{
                    "eval_type": "grammar",
                    "dataset_path": "large_stress.json",
                    "dataset_name": "large_stress"
                }]
            )
            
            execution_time = time.time() - start_time
        
        # Should handle large datasets
        assert result is True
        assert execution_time < 10.0  # Should complete within 10 seconds
        mock_eval.assert_called()
    
    def test_rapid_consecutive_evaluations(self, performance_mock_client):
        """Stress test with rapid consecutive evaluations."""
        orchestrator = EvaluationOrchestrator(
            model_manager=ModelManager(performance_mock_client),
            dataset_registry=DatasetRegistry()
        )
        
        with patch.object(DatasetRegistry, 'evaluate_dataset') as mock_eval, \
             patch('builtins.open') as mock_open, \
             patch('time.sleep'):  # Mock sleep to avoid 2s delay per model
            
            mock_eval.return_value = {
                "dataset": "rapid_test", "correct": 1, "total": 1,
                "accuracy": 100.0, "tok_per_sec": 15.0
            }
            
            start_time = time.time()
            
            # Run 20 consecutive evaluations
            results = []
            for i in range(20):
                result = orchestrator.run_evaluation(
                    model=f"perf-model-{i:03d}",
                    datasets_config=[{
                        "eval_type": "grammar",
                        "dataset_path": "rapid_test.json", 
                        "dataset_name": f"rapid_test_{i}"
                    }]
                )
                results.append(result)
            
            total_time = time.time() - start_time
        
        # All evaluations should succeed
        assert all(results)
        assert len(results) == 20
        
        # Should complete rapidly
        assert total_time < 5.0  # All 20 evaluations in under 5 seconds
        avg_time_per_eval = total_time / 20
        assert avg_time_per_eval < 0.25  # Under 250ms per evaluation
        
        print(f"Rapid evaluation stats:")
        print(f"  Total time: {total_time:.3f}s")
        print(f"  Average per eval: {avg_time_per_eval:.3f}s")
        print(f"  Evaluations per second: {20 / total_time:.1f}")


class TestResourceUtilization:
    """Test resource utilization patterns."""
    
    def test_thread_safety(self, performance_mock_client):
        """Test thread safety of core components."""
        model_manager = ModelManager(performance_mock_client)
        
        results = []
        errors = []
        
        def worker_thread(thread_id):
            try:
                for i in range(10):
                    model_name = f"perf-model-{(thread_id * 10 + i) % 20:03d}"
                    response, stats = model_manager.query_model(
                        f"Thread {thread_id} query {i}", 
                        model_name
                    )
                    results.append((thread_id, i, len(response)))
            except Exception as e:
                errors.append((thread_id, str(e)))
        
        # Start multiple worker threads
        threads = []
        for tid in range(5):
            thread = threading.Thread(target=worker_thread, args=(tid,))
            threads.append(thread)
            thread.start()
        
        # Wait for all threads to complete
        for thread in threads:
            thread.join()
        
        # Verify thread safety
        assert len(errors) == 0, f"Thread safety errors: {errors}"
        assert len(results) == 50  # 5 threads × 10 queries each
        
        # Verify all threads got valid responses
        response_lengths = [r[2] for r in results]
        assert all(length > 0 for length in response_lengths)
    
    def test_cleanup_and_resource_management(self, performance_mock_client):
        """Test proper resource cleanup."""
        import gc
        import weakref
        
        # Create orchestrator and get weak reference
        orchestrator = EvaluationOrchestrator(
            model_manager=ModelManager(performance_mock_client),
            dataset_registry=DatasetRegistry()
        )
        
        weak_ref = weakref.ref(orchestrator)
        
        # Use orchestrator
        with patch.object(DatasetRegistry, 'evaluate_dataset') as mock_eval, \
             patch('builtins.open') as mock_open, \
             patch('time.sleep'):  # Mock sleep to avoid delays
            
            mock_eval.return_value = {
                "dataset": "cleanup_test", "correct": 1, "total": 1,
                "accuracy": 100.0, "tok_per_sec": 10.0
            }
            
            result = orchestrator.run_evaluation(
                model="perf-model-001",
                datasets_config=[{
                    "eval_type": "grammar",
                    "dataset_path": "cleanup_test.json",
                    "dataset_name": "cleanup_test"
                }]
            )
        
        assert result is True
        
        # Delete orchestrator and force garbage collection
        del orchestrator
        gc.collect()
        
        # Verify object was properly cleaned up
        assert weak_ref() is None, "Orchestrator was not properly cleaned up"
